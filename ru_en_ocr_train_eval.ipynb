{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sensationadvance/sensationadvance/blob/main/ru_en_ocr_train_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aafd11a"
      },
      "source": [
        "# Скачивание датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za2oK9IoYwyP"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.yandexcloud.net/datasouls-competitions/ai-nto-final-2022/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzbuk10yaXjM"
      },
      "outputs": [],
      "source": [
        "!unzip /content/data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lERJvXMLY6QS"
      },
      "source": [
        "# Импорт"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcGEefrrhxbB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import editdistance\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import random\n",
        "from PIL import Image, ImageDraw\n",
        "import json\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoD2T0GTYvKP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b75fcf1e"
      },
      "source": [
        "#Разделим трейн датасет на обучающую и валидационную подвыборки\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HEGa-RgECO-"
      },
      "outputs": [],
      "source": [
        "labels = pd.read_csv('../input/ocr-dataset/data/train_recognition/labels.csv')\n",
        "labels_train, labels_val = train_test_split(labels, test_size=0.05, random_state=655)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62lbBTVzD3dr"
      },
      "outputs": [],
      "source": [
        "labels.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD8oM_blOrjU"
      },
      "outputs": [],
      "source": [
        "def process_image(img, n_w=256, n_h=64):\n",
        "    w, h,_ = img.shape\n",
        "    new_w = n_h\n",
        "    new_h = int(h * (new_w / w))\n",
        "    img = cv2.resize(img, (new_h, new_w))\n",
        "    w, h,_ = img.shape\n",
        "\n",
        "    if w < n_h:\n",
        "        add_zeros = np.full((n_h-w, h,3), 0)\n",
        "        img = np.concatenate((img, add_zeros))\n",
        "        w, h,_ = img.shape\n",
        "\n",
        "    if h < n_w:\n",
        "        add_zeros = np.full((w, n_w-h,3), 0)\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "        w, h,_ = img.shape\n",
        "\n",
        "    if h > n_w or w > n_h:\n",
        "        dim = (n_w,n_h)\n",
        "        img = cv2.resize(img, dim)\n",
        "\n",
        "    return img\n",
        "\n",
        "def replace_black_to_white(image):\n",
        "    brown_lo = np.array([0,0,0])\n",
        "    brown_hi = np.array([0,0,0])\n",
        "\n",
        "    # Mask image to only select browns\n",
        "    mask = cv2.inRange(image,brown_lo,brown_hi)\n",
        "\n",
        "    # Change image to red where we found brown\n",
        "    image[mask>0] = (255,255,255)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_84tiFQSlTGC"
      },
      "outputs": [],
      "source": [
        "class ExtraLinesAugmentation:\n",
        "    '''\n",
        "    Add random black lines to an image\n",
        "    Args:\n",
        "        number_of_lines (int): number of black lines to add\n",
        "        width_of_lines (int): width of lines\n",
        "    '''\n",
        "\n",
        "    def __init__(self, number_of_lines: int = 1, width_of_lines: int = 10):\n",
        "        self.number_of_lines = number_of_lines\n",
        "        self.width_of_lines = width_of_lines\n",
        "\n",
        "    def __call__(self, img):\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        for _ in range(self.number_of_lines):\n",
        "            x1 = random.randint(0, np.array(img).shape[1]); y1 = random.randint(0, np.array(img).shape[0])\n",
        "            x2 = random.randint(0, np.array(img).shape[1]); y2 = random.randint(0, np.array(img).shape[0])\n",
        "            draw.line((x1, y1, x2 + 100, y2), fill=(100, 0, 0), width=self.width_of_lines)\n",
        "\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb9J59FTLvPW"
      },
      "outputs": [],
      "source": [
        "def plot_loss_history(train_history, val_history, title='loss'):\n",
        "    plt.figure()\n",
        "    plt.title('{}'.format(title))\n",
        "    plt.plot(train_history, label='train', zorder=1)\n",
        "\n",
        "    points = np.array(val_history)\n",
        "    steps = list(range(0, len(train_history) + 1, int(len(train_history) / len(val_history))))[1:]\n",
        "\n",
        "    plt.scatter(steps, val_history, marker='+', s=180, c='orange', label='val', zorder=2)\n",
        "    plt.xlabel('train steps')\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5f5065"
      },
      "source": [
        "## 2. Зададим параметры обучения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fa07481"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "config_json = {\n",
        "    \"alphabet\": \"\"\"@ !\"%'()+,-./0123456789:;=?AEFIMNOSTW[]abcdefghiklmnopqrstuvwxyАБВГДЕЖЗИКЛМНОПРСТУХЦЧШЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№\"\"\",\n",
        "    #\"alphabet\": \"\"\"@(),-.012345:;?I[]БВГДЗИКМНОПРСТУабвгдежзийклмнопрстуфхцчшщыьэюяё\"\"\",\n",
        "    \"save_dir\": \"data/experiments/test\",\n",
        "    \"num_epochs\": 500,\n",
        "    \"image\": {\n",
        "        \"width\": 256,\n",
        "        \"height\": 64\n",
        "    },\n",
        "    \"train\": {\n",
        "        \"root_path\": \"../input/ocr-dataset/data/train_recognition/images\",\n",
        "        \"batch_size\": 64\n",
        "    },\n",
        "    \"val\": {\n",
        "        \"root_path\": \"../input/ocr-dataset/data/train_recognition/images\",\n",
        "        \"batch_size\": 128\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_8hONhPINm6"
      },
      "outputs": [],
      "source": [
        "def process_row(text):\n",
        "    text = text.replace(' ', '|')\n",
        "    text = list(text)\n",
        "    for i in range(len(text)):\n",
        "        if text[i] not in config_json['alphabet'] and text[i] != '|':\n",
        "            text[i] = '@'\n",
        "    return \" \".join(text)\n",
        "def prepare_labels(path):\n",
        "    lines = [line.rstrip() for line in open(path)]\n",
        "    arr = []\n",
        "    for line in lines:\n",
        "        arr.append([line.split('\\t')[0], line.split('\\t')[1]])\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d8253e0"
      },
      "source": [
        "## 3. Теперь определим класс датасета (torch.utils.data.Dataset) и другие вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfIdji0ixhr_"
      },
      "outputs": [],
      "source": [
        "def black2white(image):\n",
        "    lo=np.array([0,0,0])\n",
        "\n",
        "    hi=np.array([0,0,0])\n",
        "\n",
        "    mask = cv2.inRange(image, lo, hi)\n",
        "\n",
        "    image[mask>0]=(255,255,255)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81d7bd72"
      },
      "outputs": [],
      "source": [
        "# функция которая помогает объединять картинки и таргет-текст в батч\n",
        "def collate_fn(batch):\n",
        "    images, texts, enc_texts = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    text_lens = torch.LongTensor([len(text) for text in texts])\n",
        "    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n",
        "    return images, texts, enc_pad_texts, text_lens\n",
        "def collate_fn_val(batch):\n",
        "    images, texts, enc_texts = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    text_lens = torch.LongTensor([len(text) for text in texts])\n",
        "    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n",
        "    return images, texts, enc_pad_texts, text_lens\n",
        "\n",
        "\n",
        "def get_data_loader(\n",
        "    transforms, df, root_path, tokenizer, batch_size, drop_last, config, train, shuffle=False\n",
        "):\n",
        "    dataset = OCRDataset(df, root_path, tokenizer, config, train, transforms)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "    return data_loader\n",
        "\n",
        "def get_data_loader_val(\n",
        "    transforms, df, root_path, tokenizer, batch_size, drop_last, config, train, shuffle=False\n",
        "):\n",
        "    dataset = OCRDataset(df, root_path, tokenizer, config, train, transforms)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        collate_fn=collate_fn_val,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "    return data_loader\n",
        "\n",
        "def prepare_val_image(image, transform1, transform2):\n",
        "    image3 = image.astype(np.uint8)\n",
        "    image3 = Image.fromarray(image3)\n",
        "    image3 = transform1(image3)\n",
        "    image3 = np.array(image3).astype(np.int64)\n",
        "    image3 = transform2(image3)\n",
        "    return image3\n",
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, df, root_path, tokenizer, config, train=False, transform=None):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "        self.config = config\n",
        "        self.df = df\n",
        "        self.data_len = len(self.df)\n",
        "        self.train = train\n",
        "        self.train_transform = transforms.Compose([\n",
        "            ExtraLinesAugmentation(number_of_lines=3,\n",
        "                                   width_of_lines=8),\n",
        "            transforms.RandomAffine(degrees=0,\n",
        "                                    scale=(0.935, 0.935),\n",
        "                                    fillcolor=0),\n",
        "            transforms.RandomCrop((self.config['image']['height'], self.config['image']['width'])),\n",
        "            transforms.RandomRotation(degrees=(-12, 12),\n",
        "                                      fill=255),])\n",
        "        self.img_paths = []\n",
        "        self.texts = []\n",
        "        for i in range(self.data_len):\n",
        "            self.img_paths.append(os.path.join(root_path, self.df['file_name'].iloc[i]))\n",
        "            self.texts.append(self.df['text'].iloc[i])\n",
        "        self.enc_texts = tokenizer.encode(self.texts)\n",
        "    def __len__(self):\n",
        "        return self.data_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        text = self.texts[idx]\n",
        "        enc_len = 32\n",
        "        enc_text = self.enc_texts[idx][:enc_len]\n",
        "        enc_text = enc_text + [0] * (enc_len - len(enc_text))\n",
        "        enc_text = torch.LongTensor(enc_text)\n",
        "        image = black2white(cv2.imread(img_path))\n",
        "\n",
        "        if self.train:\n",
        "            #image = self.blots(image)\n",
        "            image = process_image(image,\n",
        "                                  int(self.config['image']['width'] * 1.05),\n",
        "                                  int(self.config['image']['height'] * 1.05))\n",
        "\n",
        "            image = image.astype(np.uint8)\n",
        "            image = Image.fromarray(image)\n",
        "            image = self.train_transform(image)\n",
        "            image = np.array(image).astype(np.int64)\n",
        "        else:\n",
        "            image = process_image(image, self.config['image']['width'], self.config['image']['height'])\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.train:\n",
        "            image = image ** (random.random() * 0.7 + 0.6)\n",
        "        if self.train == False:\n",
        "            return image, text, enc_text\n",
        "        else:\n",
        "            return image, text, enc_text\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ab404a"
      },
      "source": [
        "## 4. Здесь определен Токенайзер - вспопогательный класс, который преобразует текст в числа\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b50de073"
      },
      "outputs": [],
      "source": [
        "CTC_BLANK = '<BLANK>'\n",
        "\n",
        "def get_char_map(alphabet):\n",
        "    \"\"\"Make from string alphabet character2int dict.\n",
        "    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n",
        "    char_map = {value: idx + 1 for (idx, value) in enumerate(alphabet)}\n",
        "    char_map[CTC_BLANK] = 0\n",
        "    return char_map\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Class for encoding and decoding string word to sequence of int\n",
        "    (and vice versa) using alphabet.\"\"\"\n",
        "\n",
        "    def __init__(self, alphabet):\n",
        "        self.char_map = get_char_map(alphabet)\n",
        "        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n",
        "\n",
        "    def encode(self, word_list):\n",
        "        \"\"\"Returns a list of encoded words (int).\"\"\"\n",
        "        enc_words = []\n",
        "        for word in word_list:\n",
        "            enc_words.append(\n",
        "                [self.char_map[char] if char in self.char_map\n",
        "                 else 1\n",
        "                 for char in word]\n",
        "            )\n",
        "        return enc_words\n",
        "\n",
        "    def get_num_chars(self):\n",
        "        return len(self.char_map)\n",
        "\n",
        "    def decode(self, enc_word_list):\n",
        "        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n",
        "        repeating characters. Also skip out of vocabulary token.\"\"\"\n",
        "        dec_words = []\n",
        "        for word in enc_word_list:\n",
        "            word_chars = ''\n",
        "            for idx, char_enc in enumerate(word):\n",
        "                # skip if blank symbol, oov token or repeated characters\n",
        "                if (\n",
        "                    char_enc != self.char_map[CTC_BLANK]\n",
        "                    # idx > 0 to avoid selecting [-1] item\n",
        "                    and not (idx > 0 and char_enc == word[idx - 1])\n",
        "                ):\n",
        "                    word_chars += self.rev_char_map[char_enc]\n",
        "            dec_words.append(word_chars)\n",
        "        return dec_words\n",
        "    def decode_after_beam(self, enc_word_list):\n",
        "        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n",
        "        repeating characters. Also skip out of vocabulary token.\"\"\"\n",
        "        dec_words = []\n",
        "        for word in enc_word_list:\n",
        "            word_chars = ''\n",
        "            for idx, char_enc in enumerate(word):\n",
        "                word_chars += self.rev_char_map[char_enc]\n",
        "            dec_words.append(word_chars)\n",
        "        return dec_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b11eaca2"
      },
      "source": [
        "## 5. Accuracy в качестве метрики\n",
        "\n",
        "Accuracy измеряет долю предсказанных строк текста, которые полностью совпадают с таргет текстом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5c166f8"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(y_true, y_pred):\n",
        "    scores = []\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        scores.append(true == pred)\n",
        "    avg_score = np.mean(scores)\n",
        "    return avg_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7706c20d"
      },
      "source": [
        "## 6. Аугментации\n",
        "\n",
        "Здесь мы задаем базовые аугментации для модели. Вы можете написать свои или использовать готовые библиотеки типа albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd4ed911"
      },
      "outputs": [],
      "source": [
        "class Normalize:\n",
        "    def __call__(self, img):\n",
        "        img = img.astype(np.float32) / 255\n",
        "        return img\n",
        "\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, arr):\n",
        "        arr = torch.from_numpy(arr)\n",
        "        return arr\n",
        "\n",
        "\n",
        "class MoveChannels:\n",
        "    \"\"\"Move the channel axis to the zero position as required in pytorch.\"\"\"\n",
        "\n",
        "    def __init__(self, to_channels_first=True):\n",
        "        self.to_channels_first = to_channels_first\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if self.to_channels_first:\n",
        "            return np.moveaxis(image, -1, 0)\n",
        "        else:\n",
        "            return np.moveaxis(image, 0, -1)\n",
        "\n",
        "\n",
        "class ImageResize:\n",
        "    def __init__(self, height, width):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = cv2.resize(image, (self.width, self.height),\n",
        "                           interpolation=cv2.INTER_LINEAR)\n",
        "        return image\n",
        "\n",
        "\n",
        "\n",
        "def get_train_transforms(height, width):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        #ImageResize(height, width),\n",
        "\n",
        "        MoveChannels(to_channels_first=True),\n",
        "        Normalize(),\n",
        "        ToTensor()\n",
        "    ])\n",
        "    return transforms\n",
        "\n",
        "\n",
        "def get_val_transforms(height, width):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        #ImageResize(height, width),\n",
        "        MoveChannels(to_channels_first=True),\n",
        "        Normalize(),\n",
        "        ToTensor()\n",
        "    ])\n",
        "    return transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97e90f73"
      },
      "source": [
        "## 7. Здесь определяем саму модель - CRNN\n",
        "\n",
        "Подробнее об архитектуре можно почитать в статье https://arxiv.org/abs/1507.05717"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76186574"
      },
      "outputs": [],
      "source": [
        "def get_resnet34_backbone(pretrained=True):\n",
        "    m = torchvision.models.resnet34(pretrained=True)\n",
        "    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n",
        "    blocks = [input_conv, m.bn1, m.relu,\n",
        "              m.maxpool, m.layer1, m.layer2, m.layer3]\n",
        "    return nn.Sequential(*blocks)\n",
        "def get_resnet50_backbone(pretrained=True):\n",
        "    m = torchvision.models.resnet50(pretrained=True)\n",
        "    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n",
        "    blocks = [input_conv, m.bn1, m.relu,\n",
        "              m.maxpool, m.layer1, m.layer2, m.layer3]\n",
        "    return nn.Sequential(*blocks)\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers,\n",
        "            dropout=dropout, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgq6FpQHkAT6"
      },
      "outputs": [],
      "source": [
        "class CRNN_RESNET(nn.Module):\n",
        "    def __init__(\n",
        "        self, number_class_symbols, out_len=32\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = get_resnet34_backbone(pretrained=True)\n",
        "        # веса resnet34 получаются из этого гитхаба https://github.com/lolpa1n/digital-peter-ocrv\n",
        "        #self.feature_extractor.load_state_dict(torch.load('../input/ocr-resnet/resnet_ocr.pt'))\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(\n",
        "            (512, out_len))\n",
        "        self.bilstm = BiLSTM(512, 256, 2)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, number_class_symbols)\n",
        "        )\n",
        "    def forward(self, x, return_x=False):\n",
        "        feature = self.feature_extractor(x)\n",
        "        b, c, h, w = feature.size()\n",
        "        feature = feature.view(b, c * h, w)\n",
        "        feature = self.avg_pool(feature)\n",
        "        feature = feature.transpose(1, 2)\n",
        "        out = self.bilstm(feature)\n",
        "        #print(x.shape)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "\n",
        "        x1 = nn.functional.log_softmax(out, dim=2).permute(1, 0, 2)\n",
        "        if return_x:\n",
        "            return x1, out\n",
        "        else:\n",
        "            return x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6df8f95"
      },
      "source": [
        "## 8. Переходим к самому скрипту обучения - циклы трейна и валидации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "852fb92c"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def val_loop(data_loader, model, criterion, tokenizer, device):\n",
        "    acc_avg = AverageMeter()\n",
        "    loss_avg = AverageMeter()\n",
        "    error_chars = 0\n",
        "    criterion2 = nn.CrossEntropyLoss()\n",
        "    total_string = 0\n",
        "    ctc_weight = 0.9\n",
        "    for images, texts, enc_pad_texts, text_lens in tqdm(data_loader):\n",
        "        batch_size = len(texts)\n",
        "        enc_pad_texts2 = deepcopy(enc_pad_texts.view(-1)).cuda()\n",
        "        text_preds, output, output2 = predict(images, model, tokenizer, device, return_output=True)\n",
        "        output_lenghts = torch.full(\n",
        "            size=(output.size(1),),\n",
        "            fill_value=output.size(0),\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        loss1 = criterion(output, enc_pad_texts, output_lenghts, text_lens).mean()\n",
        "        output2 = output2.view(output2.shape[0] * output2.shape[1], output2.shape[2])\n",
        "        loss2 = criterion2(output2,\n",
        "                           enc_pad_texts2)\n",
        "        loss = ctc_weight * loss1 + (1.0 - ctc_weight) * loss2\n",
        "        loss_avg.update(loss.item(), batch_size)\n",
        "        for i in range(batch_size):\n",
        "            total_string += 1\n",
        "            error_chars += (editdistance.eval(text_preds[i], texts[i]) / len(texts[i]))\n",
        "            '''\n",
        "            if text_preds[i] != texts[i]:\n",
        "                print('----------------')\n",
        "                print(f'true: {texts[i]}')\n",
        "                print(f'pred: {text_preds[i]}')\n",
        "            '''\n",
        "        acc_avg.update(get_accuracy(texts, text_preds), batch_size)\n",
        "    print(f\"Val loss average: {loss_avg.avg}\")\n",
        "    print(f'Validation, acc: {acc_avg.avg:.4f}')\n",
        "    print(f\"CER: {error_chars / total_string * 100}%\")\n",
        "    #loss, cer, acc\n",
        "    return loss_avg.avg, error_chars / total_string, acc_avg.avg\n",
        "\n",
        "def train_loop(data_loader, model, criterion, optimizer, epoch, train_history=[]):\n",
        "    loss_avg = AverageMeter()\n",
        "    model.train()\n",
        "    criterion2 = nn.CrossEntropyLoss()\n",
        "    ctc_weight = 0.9\n",
        "    i = 0\n",
        "    for images, texts, enc_pad_texts, text_lens in tqdm(data_loader):\n",
        "        model.zero_grad()\n",
        "        images = images.to(DEVICE)\n",
        "        enc_pad_texts2 = deepcopy(enc_pad_texts.view(-1)).cuda()\n",
        "        batch_size = len(texts)\n",
        "        output, output2 = model(images, True)\n",
        "        output_lenghts = torch.full(\n",
        "            size=(output.size(1),),\n",
        "            fill_value=output.size(0),\n",
        "            dtype=torch.long\n",
        "        )\n",
        "        #print(output.permute(1, 0, 2).shape, enc_pad_texts.shape)\n",
        "        #enc_pad_texts2 = []\n",
        "        #second_loss = criterion2(output.permute(1, 0, 2), enc_pad_texts)\n",
        "        enc_pad_texts = enc_pad_texts.flatten()  # make 1dim, the doc says we can do it\n",
        "        enc_pad_texts = enc_pad_texts[enc_pad_texts != 0]  # drop blank dims\n",
        "\n",
        "        loss1 = criterion(output, enc_pad_texts, output_lenghts, text_lens).mean()#(criterion(output, enc_pad_texts, output_lenghts, text_lens).mean() + second_loss) / 2\n",
        "        output2 = output2.view(output2.shape[0] * output2.shape[1], output2.shape[2])\n",
        "        loss2 = criterion2(output2, enc_pad_texts2)\n",
        "        loss = ctc_weight * loss1 + (1.0 - ctc_weight) * loss2\n",
        "        loss_avg.update(loss.item(), batch_size)\n",
        "        train_history.append(loss.item())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
        "        optimizer.step()\n",
        "        if i % 100 == 0:\n",
        "            print('train_loss =', loss)\n",
        "        i += 1\n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr = param_group['lr']\n",
        "    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n",
        "    return loss_avg.avg, train_history\n",
        "\n",
        "\n",
        "def predict(images, model, tokenizer, device, return_output=False):\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "    #print(images.shape)\n",
        "    with torch.no_grad():\n",
        "        output, output2 = model(images, True)\n",
        "    #output = process_output(output)\n",
        "    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n",
        "    text_preds = tokenizer.decode(pred)\n",
        "    if return_output:\n",
        "        return text_preds, output, output2\n",
        "    else:\n",
        "        return text_preds\n",
        "\n",
        "\n",
        "def get_loaders(tokenizer, config, labels_train, labels_val):\n",
        "    train_transforms = get_train_transforms(\n",
        "        height=config['image']['height'],\n",
        "        width=config['image']['width']\n",
        "    )\n",
        "    train_loader = get_data_loader(\n",
        "        df=labels_train,\n",
        "        root_path=config['train']['root_path'],\n",
        "        transforms=train_transforms,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=config['train']['batch_size'],\n",
        "        drop_last=True,\n",
        "        config=config,\n",
        "        train=True,\n",
        "        shuffle=True\n",
        "\n",
        "    )\n",
        "    val_transforms = get_val_transforms(\n",
        "        height=config['image']['height'],\n",
        "        width=config['image']['width']\n",
        "    )\n",
        "    val_loader = get_data_loader_val(\n",
        "        df=labels_val,\n",
        "        transforms=val_transforms,\n",
        "        root_path=config['val']['root_path'],\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=config['val']['batch_size'],\n",
        "        drop_last=False,\n",
        "        config=config,\n",
        "        train=False,\n",
        "        shuffle=False\n",
        "    )\n",
        "    return train_loader, val_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6SqnyFpQCRb"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(config_json['alphabet'])\n",
        "os.makedirs(config_json['save_dir'], exist_ok=True)\n",
        "train_loader, val_loader = get_loaders(tokenizer, config_json, labels_train, labels_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz4LbkxxIdOT"
      },
      "outputs": [],
      "source": [
        "tokenizer.get_num_chars()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-0kvf4AMW6n"
      },
      "outputs": [],
      "source": [
        "model = CRNN_RESNET(tokenizer.get_num_chars(), 32)\n",
        "model.to(DEVICE)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR1-i6eGB7dI"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CTCLoss(blank=0, reduction='none', zero_infinity=True)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,\n",
        "                                  weight_decay=0.01)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer=optimizer, mode='min', factor=0.5, patience=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpE9JQD1AKtS"
      },
      "outputs": [],
      "source": [
        "best_cer = np.inf\n",
        "best_loss = np.inf\n",
        "best_epoch = 0\n",
        "train_history = []\n",
        "val_history = []\n",
        "val_loop(val_loader, model, criterion, tokenizer, DEVICE)\n",
        "for epoch in tqdm(range(config_json['num_epochs'])):\n",
        "    print(\"num of epoch\", epoch)\n",
        "    loss_avg, train_history = train_loop(train_loader, model, criterion, optimizer, epoch, train_history)\n",
        "    print('average_train_loss', loss_avg)\n",
        "    val_loss_avg, cer_avg, acc_avg = val_loop(val_loader, model, criterion, tokenizer, DEVICE)\n",
        "    val_history.append(val_loss_avg)\n",
        "    scheduler.step(cer_avg)\n",
        "    if cer_avg < best_cer:\n",
        "        best_cer = cer_avg\n",
        "        best_epoch = epoch\n",
        "        best_loss = val_loss_avg\n",
        "        model_save_path = os.path.join(\n",
        "            config_json['save_dir'], f'model-{epoch}-{cer_avg:.4f}.ckpt')\n",
        "        torch.save(model.state_dict(), '/content/' + model_save_path)\n",
        "        print('Model weights saved')\n",
        "    clear_output()\n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr = param_group['lr']\n",
        "    print(f'Current CER = {cer_avg}')\n",
        "    print(f'Current loss = {val_loss_avg}')\n",
        "    print(f'Current acc_avg = {acc_avg}')\n",
        "    print(f'Current learning rate = {lr}')\n",
        "    print('-' * 20)\n",
        "    print(f'Best CER = {best_cer}')\n",
        "    print(f'Best loss = {best_loss}')\n",
        "    print(f'Best epoch = {best_epoch}')\n",
        "    plot_loss_history(train_history, val_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhwGb9c37KcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExBXQw8F7KfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNfjz0Zw7KhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyctcdecode\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdzrb3rX0FfY",
        "outputId": "fce79d7d-f5f5-4b39-905a-948caa33082c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyctcdecode\n",
            "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from pyctcdecode) (1.26.4)\n",
            "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode)\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting hypothesis<7,>=6.14 (from pyctcdecode)\n",
            "  Downloading hypothesis-6.115.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (24.2.0)\n",
            "Collecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis<7,>=6.14->pyctcdecode)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.2.2)\n",
            "Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
            "Downloading hypothesis-6.115.5-py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, pygtrie, hypothesis, pyctcdecode\n",
            "Successfully installed hypothesis-6.115.5 pyctcdecode-0.5.0 pygtrie-2.5.0 sortedcontainers-2.4.0\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-b2ib3apk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-b2ib3apk\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 8d85329aed8506ea3672e3e208971345973ea761\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.9.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.5.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.3.6)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6355049 sha256=eff6d105f21f7f94cf7604c92f4ebea4b92d5878c09751a51da4a5595db87c6b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5yw9wobe/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=61d87920790b9962c07a5e0122618eeb08b1bcce36d6379268062c438bee1d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=313379655e135a4c8c18a87ee19337a2ea33c7db6d355bdabdeac05491b77b71\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-24.10.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.10.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAN5ugu780VK",
        "outputId": "3c37cee4-abe3-4dd5-b70c-e7566964a470"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.6/553.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184482 sha256=0c44e70f4999d94837398e1f195c19a4721d4a7db845fd7de7e73b6dafc6599e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-18pqb2v1/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.system('git clone --recursive https://github.com/parlance/ctcdecode.git')\n",
        "os.system('cd ctcdecode && pip install .')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fGN4eB0goX-",
        "outputId": "24b1c628-c8d9-4966-c60e-9e049568935e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fast-ctc-decode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU-iN8HshQXk",
        "outputId": "96176c6f-6efa-4885-9000-2872e06ae32f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fast-ctc-decode\n",
            "  Downloading fast_ctc_decode-0.3.6-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading fast_ctc_decode-0.3.6-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fast-ctc-decode\n",
            "Successfully installed fast-ctc-decode-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "#from ctcdecode import CTCBeamDecoder\n",
        "from fast_ctc_decode import beam_search\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "import logging\n",
        "import kenlm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor"
      ],
      "metadata": {
        "id": "2ZToDsqE0jxp"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(\"detectron2\")\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEGM_MODEL_PATH = \"model_final.pth\"\n",
        "OCR_MODEL_PATH = \"model374.ckpt\"\n",
        "\n",
        "\n",
        "CONFIG_JSON = {\n",
        "    \"alphabet\": \"\"\"@ !\"%'()+,-./0123456789:;=?AEFIMNOSTW[]abcdefghiklmnopqrstuvwxyАБВГДЕЖЗИКЛМНОПРСТУХЦЧШЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№\"\"\",\n",
        "    #\"image\": {\"width\": 256, \"height\": 64},\n",
        "    \"image\": {\"width\": 2560, \"height\": 1978},\n",
        "}\n",
        "\n",
        "\n",
        "def get_contours_from_mask(mask, min_area=5):\n",
        "    contours, hierarchy = cv2.findContours(\n",
        "        mask.astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n",
        "    )\n",
        "    contour_list = []\n",
        "    for contour in contours:\n",
        "        if cv2.contourArea(contour) >= min_area:\n",
        "            contour_list.append(contour)\n",
        "    return contour_list\n",
        "\n",
        "\n",
        "def get_larger_contour(contours):\n",
        "    larger_area = 0\n",
        "    larger_contour = None\n",
        "    for contour in contours:\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area > larger_area:\n",
        "            larger_contour = contour\n",
        "            larger_area = area\n",
        "    return larger_contour\n",
        "\n",
        "def black2white(image):\n",
        "\n",
        "    lo=np.array([0,0,0])\n",
        "\n",
        "    hi=np.array([0,0,0])\n",
        "\n",
        "    mask = cv2.inRange(image, lo, hi)\n",
        "\n",
        "    image[mask>0]=(255,255,255)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "# class BeamSearchDecoder:\n",
        "#     def __init__(self, labels_for_bs, model_path, alpha=0.22, beta=1.1, beam_width=10):\n",
        "#         # Remove blank token (usually index 0) from labels\n",
        "#         vocab = [label for label in labels_for_bs if label != '']\n",
        "\n",
        "#         self.decoder = CTCBeamDecoder(\n",
        "#                       list(labels_for_bs),\n",
        "#                       model_path='nto_kenlm_model10.arpa',\n",
        "#                       alpha=0.22,\n",
        "#                       beta=1.1,\n",
        "#                       cutoff_top_n=5,\n",
        "#                       cutoff_prob=1,\n",
        "#                       beam_width=10,\n",
        "#                       num_processes=4,\n",
        "#                       blank_id=0,\n",
        "#                       log_probs_input=True)\n",
        "\n",
        "\n",
        "#     def decode(self, logits):\n",
        "#         # logits shape: (batch_size, time_steps, num_classes)\n",
        "#         beam_results = self.decoder.decode_batch(logits_list=logits)\n",
        "#         return beam_results\n",
        "\n",
        "class BeamSearchDecoder:\n",
        "    def __init__(self, labels_for_bs, beam_width=10):\n",
        "        \"\"\"\n",
        "        Initializes the BeamSearchDecoder using TensorFlow's K.ctc_decode.\n",
        "\n",
        "        Parameters:\n",
        "        labels_for_bs - List of labels (vocabulary), including the blank token.\n",
        "        beam_width - Beam width for the beam search decoding.\n",
        "        \"\"\"\n",
        "        # Remove the blank token if needed (usually index 0)\n",
        "        self.labels = [label for label in labels_for_bs if label != '']\n",
        "        self.labels.append('')  # Add back the blank token at the end\n",
        "        self.beam_width = beam_width\n",
        "\n",
        "    def decode(self, logits, seq_lengths):\n",
        "        \"\"\"\n",
        "        Decodes the logits using TensorFlow's CTC beam search decoder.\n",
        "\n",
        "        Parameters:\n",
        "        logits - Tensor of shape (batch_size, time_steps, num_classes) with log probabilities.\n",
        "        seq_lengths - List or array of sequence lengths for each input in the batch.\n",
        "\n",
        "        Returns:\n",
        "        Decoded sequences and their log probability scores.\n",
        "        \"\"\"\n",
        "        # Use TensorFlow's K.ctc_decode for beam search decoding\n",
        "        decoded, log_probabilities = K.ctc_decode(\n",
        "            logits,\n",
        "            seq_lengths,\n",
        "            greedy=False,\n",
        "            beam_width=self.beam_width,\n",
        "            top_paths=1  # Return only the top path\n",
        "        )\n",
        "\n",
        "        # Check if the decoded result is a SparseTensor, then convert to dense\n",
        "        decoded_sequences = []\n",
        "        for dense_seq in decoded:\n",
        "            if isinstance(dense_seq, tf.SparseTensor):\n",
        "                decoded_sequences.append(tf.sparse.to_dense(dense_seq).numpy())\n",
        "            else:\n",
        "                raise TypeError(f\"Expected a SparseTensor, but got {type(dense_seq)}\")\n",
        "\n",
        "        return decoded_sequences, log_probabilities.numpy()\n",
        "\n",
        "    def map_int_to_labels(self, sequences):\n",
        "        \"\"\"\n",
        "        Maps the integer sequences back to their corresponding label strings.\n",
        "\n",
        "        Parameters:\n",
        "        sequences - List of sequences containing integer indices of labels.\n",
        "\n",
        "        Returns:\n",
        "        List of decoded strings corresponding to the input sequences.\n",
        "        \"\"\"\n",
        "        decoded_strings = []\n",
        "        for sequence in sequences:\n",
        "            decoded_string = ''.join([self.labels[i] for i in sequence if i < len(self.labels)])\n",
        "            decoded_strings.append(decoded_string)\n",
        "        return decoded_strings\n",
        "\n",
        "\n",
        "class SEGMpredictor:\n",
        "    def __init__(self, model_path):\n",
        "        cfg = get_cfg()\n",
        "        cfg.merge_from_file(\n",
        "            model_zoo.get_config_file(\n",
        "                \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        cfg.MODEL.WEIGHTS = model_path\n",
        "        cfg.TEST.EVAL_PERIOD = 1000\n",
        "\n",
        "        cfg.INPUT.MIN_SIZE_TRAIN = 2160\n",
        "        cfg.INPUT.MAX_SIZE_TRAIN = 3130\n",
        "\n",
        "        cfg.INPUT.MIN_SIZE_TEST = 2160\n",
        "        cfg.INPUT.MAX_SIZE_TEST = 3130\n",
        "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1\n",
        "        cfg.INPUT.FORMAT = 'BGR'\n",
        "        cfg.DATALOADER.NUM_WORKERS = 4\n",
        "        cfg.SOLVER.IMS_PER_BATCH = 3\n",
        "        cfg.SOLVER.BASE_LR = 0.01\n",
        "        cfg.SOLVER.GAMMA = 0.1\n",
        "        cfg.SOLVER.STEPS = (1500,)\n",
        "\n",
        "        cfg.SOLVER.MAX_ITER = 17000\n",
        "        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "        cfg.SOLVER.CHECKPOINT_PERIOD = cfg.TEST.EVAL_PERIOD\n",
        "        cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
        "        cfg.OUTPUT_DIR = './output'\n",
        "\n",
        "        self.predictor = DefaultPredictor(cfg)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        outputs = self.predictor(img)\n",
        "        prediction = outputs[\"instances\"].pred_masks.cpu().numpy()\n",
        "        contours = []\n",
        "        for pred in prediction:\n",
        "            contour_list = get_contours_from_mask(pred)\n",
        "            contours.append(get_larger_contour(contour_list))\n",
        "        return contours\n",
        "\n",
        "\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "CTC_BLANK = \"<BLANK>\"\n",
        "\n",
        "\n",
        "def get_char_map(alphabet):\n",
        "    \"\"\"Make from string alphabet character2int dict.\n",
        "    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n",
        "    char_map = {value: idx + 1 for (idx, value) in enumerate(alphabet)}\n",
        "    char_map[CTC_BLANK] = 0\n",
        "    return char_map\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Class for encoding and decoding string word to sequence of int\n",
        "    (and vice versa) using alphabet.\"\"\"\n",
        "\n",
        "    def __init__(self, alphabet):\n",
        "        self.char_map = get_char_map(alphabet)\n",
        "        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n",
        "\n",
        "    def encode(self, word_list):\n",
        "        \"\"\"Returns a list of encoded words (int).\"\"\"\n",
        "        enc_words = []\n",
        "        for word in word_list:\n",
        "            enc_words.append(\n",
        "                [self.char_map[char] if char in self.char_map\n",
        "                 else 1\n",
        "                 for char in word]\n",
        "            )\n",
        "        return enc_words\n",
        "\n",
        "    def get_num_chars(self):\n",
        "        return len(self.char_map)\n",
        "\n",
        "    def decode(self, enc_word_list):\n",
        "        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n",
        "        repeating characters. Also skip out of vocabulary token.\"\"\"\n",
        "        dec_words = []\n",
        "        for word in enc_word_list:\n",
        "            word_chars = ''\n",
        "            for idx, char_enc in enumerate(word):\n",
        "                # skip if blank symbol, oov token or repeated characters\n",
        "                if (\n",
        "                    char_enc != self.char_map[CTC_BLANK]\n",
        "                    # idx > 0 to avoid selecting [-1] item\n",
        "                    and not (idx > 0 and char_enc == word[idx - 1])\n",
        "                ):\n",
        "                    word_chars += self.rev_char_map[char_enc]\n",
        "            dec_words.append(word_chars)\n",
        "        return dec_words\n",
        "\n",
        "    def decode_after_beam(self, enc_word_list):\n",
        "        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n",
        "        repeating characters. Also skip out of vocabulary token.\"\"\"\n",
        "        dec_words = []\n",
        "        for word in enc_word_list:\n",
        "            word_chars = ''\n",
        "            for idx, char_enc in enumerate(word):\n",
        "                word_chars += self.rev_char_map[char_enc]\n",
        "            dec_words.append(word_chars)\n",
        "        return dec_words\n",
        "\n",
        "class Normalize:\n",
        "    def __call__(self, img):\n",
        "        img = img.astype(np.float32) / 255\n",
        "        return img\n",
        "\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, arr):\n",
        "        arr = torch.from_numpy(arr)\n",
        "        return arr\n",
        "\n",
        "\n",
        "class MoveChannels:\n",
        "    \"\"\"Move the channel axis to the zero position as required in pytorch.\"\"\"\n",
        "\n",
        "    def __init__(self, to_channels_first=True):\n",
        "        self.to_channels_first = to_channels_first\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if self.to_channels_first:\n",
        "            return np.moveaxis(image, -1, 0)\n",
        "        else:\n",
        "            return np.moveaxis(image, 0, -1)\n",
        "\n",
        "\n",
        "class ImageResize:\n",
        "    def __init__(self, height, width):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = cv2.resize(\n",
        "            image, (self.width, self.height), interpolation=cv2.INTER_LINEAR\n",
        "        )\n",
        "        return image\n",
        "\n",
        "\n",
        "def get_val_transforms(height, width):\n",
        "    transforms = torchvision.transforms.Compose(\n",
        "        [\n",
        "            MoveChannels(to_channels_first=True),\n",
        "            Normalize(),\n",
        "            ToTensor(),\n",
        "        ]\n",
        "    )\n",
        "    return transforms\n",
        "\n",
        "\n",
        "def get_resnet34_backbone(pretrained=True):\n",
        "    m = torchvision.models.resnet34(pretrained=pretrained)\n",
        "    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n",
        "    blocks = [input_conv, m.bn1, m.relu,\n",
        "              m.maxpool, m.layer1, m.layer2, m.layer3]\n",
        "    return nn.Sequential(*blocks)\n",
        "\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers,\n",
        "            dropout=dropout, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self, number_class_symbols\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = get_resnet34_backbone(pretrained=False)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(\n",
        "            (512, 32))\n",
        "        self.bilstm = BiLSTM(512, 256, 2)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, number_class_symbols)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.view(b, c * h, w)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.bilstm(x)\n",
        "        x = self.classifier(x)\n",
        "        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "def predict(images, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(images)\n",
        "    return output\n",
        "\n",
        "\n",
        "class InferenceTransform:\n",
        "    def __init__(self, height, width):\n",
        "        self.transforms = get_val_transforms(height, width)\n",
        "\n",
        "    def __call__(self, images):\n",
        "        transformed_images = []\n",
        "        for image in images:\n",
        "            image = self.transforms(image)\n",
        "            transformed_images.append(image)\n",
        "        transformed_tensor = torch.stack(transformed_images, 0)\n",
        "        return transformed_tensor\n",
        "\n",
        "\n",
        "def process_image(img, n_w=256, n_h=64):\n",
        "    # img = prepare_image(img)\n",
        "    w, h, _ = img.shape\n",
        "\n",
        "    new_w = n_h\n",
        "    new_h = int(h * (new_w / w))\n",
        "    img = cv2.resize(img, (new_h, new_w))\n",
        "    w, h, _ = img.shape\n",
        "\n",
        "    # img = img.astype('float32')\n",
        "\n",
        "    if w < n_h:\n",
        "        add_zeros = np.full((n_h - w, h, 3), 0)\n",
        "        img = np.concatenate((img, add_zeros))\n",
        "        w, h, _ = img.shape\n",
        "\n",
        "    if h < n_w:\n",
        "        add_zeros = np.full((w, n_w - h, 3), 0)\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "        w, h, _ = img.shape\n",
        "\n",
        "    if h > n_w or w > n_h:\n",
        "        dim = (n_w, n_h)\n",
        "        img = cv2.resize(img, dim)\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "class OcrPredictor:\n",
        "    def __init__(self, model_path, config, device=\"cuda\"):\n",
        "        self.tokenizer = Tokenizer(config[\"alphabet\"])\n",
        "        self.device = torch.device(device)\n",
        "        # load model\n",
        "        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.transforms = InferenceTransform(\n",
        "            height=config[\"image\"][\"height\"],\n",
        "            width=config[\"image\"][\"width\"],\n",
        "        )\n",
        "        labels_for_bs = \"\"\"_@|!\"%'()+,-./0123456789:;=?AEFIMNOSTW[]abcdefghiklmnopqrstuvwxyАБВГДЕЖЗИКЛМНОПРСТУХЦЧШЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№\"\"\"\n",
        "        # self.decoder = CTCBeamDecoder(\n",
        "        #        list(labels_for_bs),\n",
        "        #        model_path='nto_kenlm_model10.arpa',\n",
        "        #        alpha=0.22,\n",
        "        #        beta=1.1,\n",
        "        #        cutoff_top_n=5,\n",
        "        #        cutoff_prob=1,\n",
        "        #        beam_width=10,\n",
        "        #        num_processes=4,\n",
        "        #        blank_id=0,\n",
        "        #        log_probs_input=True)\n",
        "        self.decoder = BeamSearchDecoder(\n",
        "            labels_for_bs=list(labels_for_bs),  # Label list (including blank token)\n",
        "            beam_width=10  # Beam width for the decoding process\n",
        "        )\n",
        "\n",
        "    def __call__(self, images):\n",
        "        if isinstance(images, (list, tuple)):\n",
        "            one_image = False\n",
        "        elif isinstance(images, np.ndarray):\n",
        "            images = images\n",
        "            one_image = True\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"Input must contain np.ndarray, \"\n",
        "                f\"tuple or list, found {type(images)}.\"\n",
        "            )\n",
        "        images = black2white(images)\n",
        "        images = [process_image(images)]\n",
        "        images = self.transforms(images)\n",
        "        # output = predict(images, self.model, self.tokenizer, self.device)\n",
        "        # beam_results, beam_scores, timesteps, out_lens = self.decoder.decode(output.permute(1, 0, 2))\n",
        "        # encoded_text = beam_results[0][0][:out_lens[0][0]]\n",
        "        # text_pred = self.tokenizer.decode_after_beam([encoded_text.numpy()])[0]\n",
        "        # Get model output (logits)\n",
        "        output = predict(images, self.model, self.tokenizer, self.device)\n",
        "\n",
        "        # Permute the logits to match TensorFlow's expected shape: (batch_size, time_steps, num_classes)\n",
        "        logits = output.permute(1, 0, 2).cpu().detach().numpy()  # Convert to numpy if needed\n",
        "\n",
        "        # Sequence lengths (assuming the full length for all sequences)\n",
        "        seq_lengths = [logits.shape[1]] * logits.shape[0]  # Length is the time_steps dimension\n",
        "\n",
        "        # Decode using TensorFlow's beam search decoder\n",
        "        decoded_sequences, log_probabilities = self.decoder.decode(logits, seq_lengths)\n",
        "\n",
        "        # Map the decoded sequences back to label strings\n",
        "        decoded_strings = self.decoder.map_int_to_labels(decoded_sequences)\n",
        "\n",
        "        # Since we only have the top path, take the first decoded string\n",
        "        text_pred = decoded_strings[0]\n",
        "        return text_pred\n",
        "\n",
        "        \"\"\"\n",
        "        # Preprocess images\n",
        "images = black2white(images)\n",
        "images = [process_image(images)]\n",
        "images = self.transforms(images)\n",
        "\n",
        "# Get model output (logits)\n",
        "output = predict(images, self.model, self.tokenizer, self.device)\n",
        "\n",
        "# Permute the logits to match TensorFlow's expected shape: (batch_size, time_steps, num_classes)\n",
        "logits = output.permute(1, 0, 2).cpu().detach().numpy()  # Convert to numpy if needed\n",
        "\n",
        "# Sequence lengths (assuming the full length for all sequences)\n",
        "seq_lengths = [logits.shape[1]] * logits.shape[0]  # Length is the time_steps dimension\n",
        "\n",
        "# Decode using TensorFlow's beam search decoder\n",
        "decoded_sequences, log_probabilities = self.decoder.decode(logits, seq_lengths)\n",
        "\n",
        "# Map the decoded sequences back to label strings\n",
        "decoded_strings = self.decoder.map_int_to_labels(decoded_sequences)\n",
        "\n",
        "# Since we only have the top path, take the first decoded string\n",
        "text_pred = decoded_strings[0]\n",
        "        \"\"\"\n",
        "\n",
        "def get_image_visualization(img, pred_data, fontpath, font_koef=50):\n",
        "    h, w = img.shape[:2]\n",
        "    font = ImageFont.truetype(fontpath, int(h / font_koef))\n",
        "    empty_img = Image.new(\"RGB\", (w, h), (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(empty_img)\n",
        "\n",
        "    for prediction in pred_data[\"predictions\"]:\n",
        "        polygon = prediction[\"polygon\"]\n",
        "        pred_text = prediction[\"text\"]\n",
        "        cv2.drawContours(img, np.array([polygon]), -1, (0, 255, 0), 2)\n",
        "        x, y, w, h = cv2.boundingRect(np.array([polygon]))\n",
        "        draw.text((x, y), pred_text, fill=0, font=font)\n",
        "\n",
        "    vis_img = np.array(empty_img)\n",
        "    vis = np.concatenate((img, vis_img), axis=1)\n",
        "    return vis\n",
        "\n",
        "\n",
        "def crop_img_by_polygon(img, polygon):\n",
        "    pts = np.array(polygon)\n",
        "    rect = cv2.boundingRect(pts)\n",
        "    x, y, w, h = rect\n",
        "    croped = img[y : y + h, x : x + w].copy()\n",
        "    pts = pts - pts.min(axis=0)\n",
        "    mask = np.zeros(croped.shape[:2], np.uint8)\n",
        "    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
        "    dst = cv2.bitwise_and(croped, croped, mask=mask)\n",
        "    return dst\n",
        "\n",
        "\n",
        "class PiepleinePredictor:\n",
        "    def __init__(self, segm_model_path, ocr_model_path, ocr_config):\n",
        "        self.segm_predictor = SEGMpredictor(model_path=segm_model_path)\n",
        "        self.ocr_predictor = OcrPredictor(model_path=ocr_model_path, config=ocr_config)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        output = {\"predictions\": []}\n",
        "        contours = self.segm_predictor(img)\n",
        "        for contour in contours:\n",
        "            if contour is not None:\n",
        "                crop = crop_img_by_polygon(img, contour)\n",
        "                pred_text = self.ocr_predictor(crop)\n",
        "                output[\"predictions\"].append(\n",
        "                    {\n",
        "                        \"polygon\": [[int(i[0][0]), int(i[0][1])] for i in contour],\n",
        "                        \"text\": pred_text,\n",
        "                    }\n",
        "                )\n",
        "        return output\n",
        "\n",
        "\n",
        "def main():\n",
        "    pipeline_predictor = PiepleinePredictor(\n",
        "        segm_model_path=SEGM_MODEL_PATH,\n",
        "        ocr_model_path=OCR_MODEL_PATH,\n",
        "        ocr_config=CONFIG_JSON,\n",
        "    )\n",
        "    pred_data = {}\n",
        "    image = cv2.imread(\"1_pass_4.png\")\n",
        "    pred_data[\"1_pass_4.png\"] = pipeline_predictor(image)\n",
        "    print(pred_data)\n",
        "\n",
        "    # for img_name in tqdm(os.listdir(TEST_IMAGES_PATH)):\n",
        "    #     image = cv2.imread(os.path.join(TEST_IMAGES_PATH, img_name))\n",
        "    #     pred_data[img_name] = pipeline_predictor(image)\n",
        "\n",
        "    # with open(SAVE_PATH, \"w\") as f:\n",
        "    #     json.dump(pred_data, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "_O2FGHvEzJUW",
        "outputId": "e28ff6e6-b195-4a87-f34f-d18df0a11a71"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Expected a SparseTensor, but got <class 'tensorflow.python.framework.ops.EagerTensor'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-c38dfe8caecf>\u001b[0m in \u001b[0;36m<cell line: 547>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-c38dfe8caecf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0mpred_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1_pass_4.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m     \u001b[0mpred_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1_pass_4.png\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-c38dfe8caecf>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontour\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_img_by_polygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontour\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0mpred_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mocr_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m                 output[\"predictions\"].append(\n\u001b[1;32m    520\u001b[0m                     {\n",
            "\u001b[0;32m<ipython-input-64-c38dfe8caecf>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# Decode using TensorFlow's beam search decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mdecoded_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# Map the decoded sequences back to label strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-c38dfe8caecf>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, logits, seq_lengths)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mdecoded_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a SparseTensor, but got {type(dense_seq)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected a SparseTensor, but got <class 'tensorflow.python.framework.ops.EagerTensor'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.0rc0 keras==2.8.0rc0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W36YMwS1EvIV",
        "outputId": "22177e85-d633-4a16-ef12-503ea11d1037"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (2.8.0rc0)\n",
            "Requirement already satisfied: keras==2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (2.8.0rc0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (3.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (3.20.3)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (59.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.8,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (2.7.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0rc0) (1.64.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0rc0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.8,>=2.7->tensorflow==2.8.0rc0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "import editdistance\n",
        "\n",
        "print(os.path.abspath(os.getcwd()))\n",
        "WORKING_DIR = os.path.join('../')\n",
        "\n",
        "r'''\n",
        "params:\n",
        "\n",
        "callbacks: list of callback names\n",
        "metrics: list of metric names\n",
        "checkpoint_path: str path for checkpoints\n",
        "csv_log_path: str path for csv logs\n",
        "tb_log_path: str path for files to be parsed by TensorBoard\n",
        "tb_update_freq: 'batch'/'epoch'/int frequency of writing to TensorBoard\n",
        "epochs: int number of epochs\n",
        "batch_size: int size of batch\n",
        "early_stopping_patience: int early stopping patience\n",
        "input_img_shape: array(width, height, 1)\n",
        "vocab_len: int length of vocabulary with blank\n",
        "max_label_len: int max length of labels\n",
        "chars_path: path to file that contains alphabet\n",
        "blank: str blank symbol for ctc\n",
        "'''\n",
        "\n",
        "class CTCLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, blank_index, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
        "        self.blank_index = blank_index\n",
        "\n",
        "    def get_config(self):\n",
        "        return super().get_config()\n",
        "\n",
        "    def cer(self, y_true, y_pred, pred_sequence_length, true_sequence_length):\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "\n",
        "        pred_codes_dense = K.ctc_decode(y_pred, tf.squeeze(pred_sequence_length, axis=-1), greedy=True)\n",
        "        # -1 - blank in ctc_decode\n",
        "\n",
        "        pred_codes_dense = tf.squeeze(tf.cast(pred_codes_dense[0], tf.int64), axis=0)  # only [0] if greedy=true\n",
        "        idx = tf.where(tf.not_equal(pred_codes_dense, -1))\n",
        "        pred_codes_sparse = tf.SparseTensor(tf.cast(idx, tf.int64),\n",
        "                                            tf.gather_nd(pred_codes_dense, idx),\n",
        "                                            tf.cast(tf.shape(pred_codes_dense), tf.int64))\n",
        "\n",
        "        idx = tf.where(tf.not_equal(y_true, self.blank_index))\n",
        "        label_sparse = tf.SparseTensor(tf.cast(idx, tf.int64),\n",
        "                                       tf.gather_nd(y_true, idx),\n",
        "                                       tf.cast(tf.shape(y_true), tf.int64))\n",
        "        label_sparse = tf.cast(label_sparse, tf.int64)\n",
        "\n",
        "        distances = tf.reduce_sum(tf.edit_distance(pred_codes_sparse, label_sparse, normalize=False))\n",
        "\n",
        "        # compute chars amount represent in y_true\n",
        "        count_chars = len(idx)\n",
        "\n",
        "        return tf.divide(tf.cast(distances, tf.float32), tf.cast(count_chars, tf.float32), name='CER')\n",
        "\n",
        "    def accuracy(self, y_true, y_pred, pred_sequence_length, true_sequence_length):\n",
        "        batch_len = tf.shape(y_true)[0]\n",
        "\n",
        "        pred_codes_dense = K.ctc_decode(y_pred, tf.squeeze(pred_sequence_length, axis=-1), greedy=True)\n",
        "        # -1 - blank in ctc_decode\n",
        "\n",
        "        pred_codes_dense = tf.squeeze(tf.cast(pred_codes_dense[0], tf.int64), axis=0)  # only [0] if greedy=true\n",
        "        idx = tf.where(tf.not_equal(pred_codes_dense, -1))\n",
        "        pred_codes_sparse = tf.SparseTensor(tf.cast(idx, tf.int64),\n",
        "                                            tf.gather_nd(pred_codes_dense, idx),\n",
        "                                            tf.cast(tf.shape(pred_codes_dense), tf.int64))\n",
        "\n",
        "        idx = tf.where(tf.not_equal(y_true, self.blank_index))\n",
        "        label_sparse = tf.SparseTensor(tf.cast(idx, tf.int64),\n",
        "                                       tf.gather_nd(y_true, idx),\n",
        "                                       tf.cast(tf.shape(y_true), tf.int64))\n",
        "        label_sparse = tf.cast(label_sparse, tf.int64)\n",
        "\n",
        "        correct_words_amount = len(\n",
        "            tf.where(tf.equal(tf.edit_distance(pred_codes_sparse, label_sparse, normalize=False), 0))\n",
        "        )\n",
        "\n",
        "        return tf.divide(tf.cast(correct_words_amount, tf.float32), tf.cast(batch_len, tf.float32), name='accuracy')\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        cer = self.cer(y_true, y_pred, input_length, label_length)\n",
        "        accuracy = self.accuracy(y_true, y_pred, input_length, label_length)\n",
        "        self.add_metric(cer, name='cer')\n",
        "        self.add_metric(accuracy, name='accuracy')\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "class Speller():\n",
        "    def __init__(self, char_list, corpus_file):\n",
        "        corpus = open(corpus_file, encoding='utf8').read()\n",
        "        self.chars = ''.join(char_list)\n",
        "        self.non_letter = ''.join(c for c in char_list if not self.__is_cyrillic(c))\n",
        "        self.dictionary = self.__get_dictionary(corpus)\n",
        "\n",
        "    def __is_cyrillic(self, char):\n",
        "        return bool(re.match('[а-яА-ЯёЁ]', char))\n",
        "\n",
        "    def __get_dictionary(self, corpus):\n",
        "        non_letter_re = '[' + self.non_letter.replace(' ', '') + ']'\n",
        "        non_letter_re = non_letter_re.replace('[UNK]', '')\n",
        "        corpus = re.sub('[!(),-.:;?#]', '', corpus).lower()\n",
        "        dictionary = set(str.split(corpus))\n",
        "        return dictionary\n",
        "\n",
        "    def __get_closest_word(self, word, min_dist_coef):\n",
        "        flag = word[0].isupper()\n",
        "        res_word = word = word.lower()\n",
        "        min_dist = min_dist_coef * len(word)\n",
        "        for dict_word in self.dictionary:\n",
        "            dist = editdistance.eval(word, dict_word)\n",
        "            if dist == 0:\n",
        "                return word.capitalize() if flag else word\n",
        "            elif dist < min_dist:\n",
        "                min_dist = dist\n",
        "                res_word = dict_word\n",
        "        return res_word.capitalize() if flag else res_word\n",
        "\n",
        "    def __compute_label(self, label, min_dist_coef):\n",
        "        start_i = -1\n",
        "        res_label = ''\n",
        "        for i in range(len(label)):\n",
        "            if label[i] in self.non_letter:\n",
        "                if start_i >= 0:\n",
        "                    res_label += self.__get_closest_word(label[start_i:i], min_dist_coef)\n",
        "                    start_i = -1\n",
        "                res_label += label[i]\n",
        "            elif start_i < 0:\n",
        "                start_i = i\n",
        "        if start_i >= 0:\n",
        "            res_label += self.__get_closest_word(label[start_i:i+1], min_dist_coef)\n",
        "        return res_label\n",
        "\n",
        "    def compute_batch(self, labels, min_dist_coef=0.3):\n",
        "        return [self.__compute_label(label, min_dist_coef) for label in labels]\n",
        "\n",
        "    def compute_img(self, label, min_dist_coef=0.3):\n",
        "        return self.__compute_label(label, min_dist_coef)\n",
        "\n",
        "class Model():\n",
        "\n",
        "    def __init__(self, params):\n",
        "        self.callbacks = []\n",
        "        self.epochs = params['epochs']\n",
        "        self.metrics = params['metrics']\n",
        "        self.history = dict()\n",
        "\n",
        "        self.model = None\n",
        "        self.pred_model = None\n",
        "\n",
        "        self.input_shape = params['input_img_shape']\n",
        "        self.batch_size = params['batch_size']\n",
        "        self.vocab_len = params['vocab_len']\n",
        "        self.max_label_len = params['max_label_len']\n",
        "        self.chars_path = params['chars_path']\n",
        "        self.blank = params['blank']\n",
        "        self.blank_index = None\n",
        "\n",
        "        self.vocab = None\n",
        "        self.num_to_char = None\n",
        "        self.char_to_num = None\n",
        "        self.__set_mapping()\n",
        "        self.speller = Speller(self.vocab, params['corpus'])\n",
        "\n",
        "\n",
        "        if 'checkpoint' in params['callbacks']:\n",
        "            self.cp_path = params['checkpoint_path']\n",
        "\n",
        "            cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.cp_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only=True,\n",
        "                                                 monitor='val_loss',\n",
        "                                                 mode='min',\n",
        "                                                 verbose=1)\n",
        "            self.callbacks.append(cp_callback)\n",
        "\n",
        "        if 'csv_log' in params['callbacks']:\n",
        "            self.csv_log_path = params['csv_log_path']\n",
        "            csv_log_callback = tf.keras.callbacks.CSVLogger(self.csv_log_path,\n",
        "                                                      append=False, separator=';')\n",
        "            self.callbacks.append(csv_log_callback)\n",
        "\n",
        "        if 'tb_log' in params['callbacks']:\n",
        "            self.tb_log_path = params['tb_log_path']\n",
        "            tb_log_callback = tf.keras.callbacks.TensorBoard(log_dir=self.tb_log_path,\n",
        "                                                             update_freq=params['tb_update_freq'])\n",
        "            self.callbacks.append(tb_log_callback)\n",
        "\n",
        "        if 'early_stopping' in params['callbacks']:\n",
        "            early_stopping = keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_loss\", patience=params['early_stopping_patience'], restore_best_weights=True\n",
        "            )\n",
        "            self.callbacks.append(early_stopping)\n",
        "\n",
        "    def __set_mapping(self):\n",
        "        self.vocab = open(self.chars_path, encoding=\"utf8\").read().split(\"\\n\")\n",
        "        # Mapping characters to integers\n",
        "        self.char_to_num = layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=self.vocab, mask_token=None\n",
        "        )\n",
        "        # Mapping integers back to original characters\n",
        "        self.num_to_char = layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=self.char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
        "        )\n",
        "        self.blank_index = self.char_to_num(tf.strings.unicode_split(self.blank, input_encoding=\"UTF-8\")).numpy()[0]\n",
        "\n",
        "    def load_weights(self, path):\n",
        "        self.model.load_weights(path)\n",
        "\n",
        "        self.pred_model = keras.models.Model(\n",
        "            self.model.get_layer(name='image').input, self.model.get_layer(name='dense2').output\n",
        "        )\n",
        "\n",
        "    def build(self):\n",
        "        self.__set_input()\n",
        "        self.__set_CNN()\n",
        "        self.__set_RNN()\n",
        "        self.__set_output()\n",
        "\n",
        "        self.model = keras.models.Model(\n",
        "            inputs=[self.input, self.labels], outputs=self.output, name=\"htr_model\"\n",
        "        )\n",
        "\n",
        "        # Optimizer\n",
        "        opt = keras.optimizers.Adam()\n",
        "\n",
        "        # Compile the model\n",
        "        self.model.compile(\n",
        "            optimizer=opt,\n",
        "        )\n",
        "\n",
        "    def __set_input(self):\n",
        "        self.input = layers.Input(\n",
        "            shape=self.input_shape, name='image', dtype='float32'\n",
        "        )\n",
        "        self.labels = layers.Input(name='label', shape=(None, ), dtype='float32')\n",
        "\n",
        "    def __set_CNN(self):\n",
        "        self.x = layers.Conv2D(\n",
        "            32,\n",
        "            (5, 5),\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=\"he_normal\",\n",
        "            padding=\"same\",\n",
        "            name=\"Conv1\",\n",
        "        )(self.input)\n",
        "        self.x = layers.MaxPooling2D((2, 2), name=\"pool1\")(self.x)\n",
        "\n",
        "        self.x = layers.Conv2D(\n",
        "            64,\n",
        "            (3, 3),\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=\"he_normal\",\n",
        "            padding=\"same\",\n",
        "            name=\"Conv2\",\n",
        "        )(self.x)\n",
        "        self.x = layers.MaxPooling2D((2, 2), name=\"pool2\")(self.x)\n",
        "\n",
        "        self.x = layers.Conv2D(\n",
        "            128,\n",
        "            (3, 3),\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=\"he_normal\",\n",
        "            padding=\"same\",\n",
        "            name=\"Conv3\",\n",
        "        )(self.x)\n",
        "        self.x = layers.MaxPooling2D((2, 2), name=\"pool3\")(self.x)\n",
        "\n",
        "        self.x = layers.Conv2D(\n",
        "            256,\n",
        "            (2, 2),\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=\"he_normal\",\n",
        "            padding=\"same\",\n",
        "            name=\"Conv4\",\n",
        "        )(self.x)\n",
        "\n",
        "        new_shape = ((self.input_shape[0] // 8), (self.input_shape[1] // 8) * 256)\n",
        "        self.x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(self.x)\n",
        "        self.x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(self.x)\n",
        "        self.x = layers.Dropout(0.2)(self.x)\n",
        "\n",
        "    def __set_RNN(self):\n",
        "        self.x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(self.x)\n",
        "        self.x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(self.x)\n",
        "\n",
        "    def __set_output(self):\n",
        "        self.x = layers.Dense(\n",
        "            self.vocab_len, activation=\"softmax\", name=\"dense2\"\n",
        "        )(self.x)\n",
        "        self.output = CTCLayer(self.blank_index, name=\"ctc_loss\")(self.labels, self.x)\n",
        "\n",
        "    def get_summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def fit(self, train, val):\n",
        "        self.history = self.model.fit(\n",
        "            train,\n",
        "            validation_data=val,\n",
        "            epochs=self.epochs,\n",
        "            callbacks=self.callbacks,\n",
        "        )\n",
        "\n",
        "        self.pred_model = keras.models.Model(\n",
        "            self.model.get_layer(name='image').input, self.model.get_layer(name='dense2').output\n",
        "        )\n",
        "\n",
        "        print(f'\\n\\nmodel weights saved at {self.cp_path}\\n\\n')\n",
        "\n",
        "    def decode_batch_predictions(self, pred):\n",
        "        input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "        results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
        "                  :, :self.max_label_len\n",
        "                  ]\n",
        "\n",
        "        # Iterate over the results and get back the text\n",
        "        output_text = []\n",
        "        for res in results:\n",
        "            res = tf.strings.reduce_join(self.num_to_char(res)).numpy().decode(\"utf-8\").replace('[UNK]', \"\")\n",
        "            output_text.append(res)\n",
        "\n",
        "        return output_text\n",
        "\n",
        "    def predict(self, batch):\n",
        "        batch_images = batch['image']\n",
        "\n",
        "        pred = self.pred_model.predict(batch_images)\n",
        "        pred_texts = self.decode_batch_predictions(pred)\n",
        "\n",
        "        return pred_texts\n",
        "\n",
        "    def __strided_rescale(self, img, bin_fac):\n",
        "        strided = as_strided(img, shape=(img.shape[0]//bin_fac, img.shape[1]//bin_fac, bin_fac, bin_fac),\n",
        "                             strides=((img.strides[0]*bin_fac, img.strides[1]*bin_fac)+img.strides))\n",
        "        return strided.mean(axis=-1).mean(axis=-1)\n",
        "\n",
        "    def __resize_img(self, img, new_img_height, new_img_width):\n",
        "        img_size = np.array(img.shape[:2])\n",
        "        new_img_size = np.array([new_img_height, new_img_width])\n",
        "        diff = img_size - new_img_size\n",
        "        h_ratio = w_ratio = 0\n",
        "        if diff[0] > 0:\n",
        "            h_ratio = img_size[0] / new_img_size[0]\n",
        "        if diff[1] > 0:\n",
        "            w_ratio = img_size[0] / new_img_size[0]\n",
        "        if h_ratio != 0 or w_ratio != 0:\n",
        "            ratio = round(max(h_ratio, w_ratio))\n",
        "            img = self.__strided_rescale(img, ratio)\n",
        "        return img\n",
        "\n",
        "    def __apply_brightness_contrast(self, input_img, brightness=0, contrast=0):\n",
        "        if brightness != 0:\n",
        "            if brightness > 0:\n",
        "                shadow = brightness\n",
        "                highlight = 255\n",
        "            else:\n",
        "                shadow = 0\n",
        "                highlight = 255 + brightness\n",
        "            alpha_b = (highlight - shadow)/255\n",
        "            gamma_b = shadow\n",
        "\n",
        "            buf = cv2.addWeighted(input_img, alpha_b, input_img, 0, gamma_b)\n",
        "        else:\n",
        "            buf = input_img.copy()\n",
        "\n",
        "        if contrast != 0:\n",
        "            f = 131*(contrast + 127)/(127*(131-contrast))\n",
        "            alpha_c = f\n",
        "            gamma_c = 127*(1-f)\n",
        "\n",
        "            buf = cv2.addWeighted(buf, alpha_c, buf, 0, gamma_c)\n",
        "\n",
        "        return buf\n",
        "\n",
        "    def __encode_img(self, img):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mean_val = img.mean()\n",
        "        if mean_val < 230:\n",
        "            img = self.__apply_brightness_contrast(img, 230/mean_val*60, 230/mean_val*30)\n",
        "        img = self.__resize_img(img, self.input_shape[1], self.input_shape[0]).astype(np.uint8)\n",
        "        img = np.expand_dims(img, 2)\n",
        "        img = tf.convert_to_tensor(img)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        img = 1 - img\n",
        "        img = tf.image.resize_with_pad(img, self.input_shape[1], self.input_shape[0])\n",
        "        img = 0.5 - img\n",
        "        img = tf.transpose(img, perm=[1, 0, 2])\n",
        "        return tf.expand_dims(img, 0)\n",
        "\n",
        "    def predict_img(self, img): #img is np.ndarray of shape (height, width, 3) / (height, width, 1) - rgb, gray\n",
        "        try:\n",
        "            if len(img.shape) == 2:\n",
        "                img = np.stack((img,)*3, axis=-1)\n",
        "            img = self.__encode_img(img)\n",
        "            pred = self.pred_model.predict(img)\n",
        "            pred_text = self.decode_batch_predictions(pred)\n",
        "            return self.speller.compute_img(pred_text[0])\n",
        "        except ValueError:\n",
        "            return \"Error: Incorrect photo\"\n",
        "\n",
        "    def evaluate(self, batch):\n",
        "        return self.model.evaluate(batch)\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history.history\n",
        "\n",
        "\n",
        "img_width = 900\n",
        "img_height = 120\n",
        "\n",
        "# parameters of resized images\n",
        "new_img_width = 350\n",
        "new_img_height = 50\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "logging.basicConfig(format=u'%(filename)s [ LINE:%(lineno)+3s ]#%(levelname)+8s [%(asctime)s]  %(message)s',\n",
        "                    level=logging.INFO)\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    'callbacks': ['checkpoint', 'csv_log', 'tb_log', 'early_stopping'],\n",
        "    'metrics': ['cer', 'accuracy'],\n",
        "    'checkpoint_path': 'cp.ckpt',\n",
        "    'csv_log_path': 'log_2.csv',\n",
        "    'tb_log_path': 'log2',\n",
        "    'tb_update_freq': 200,\n",
        "    'epochs': 50,\n",
        "    'batch_size': batch_size,\n",
        "    'early_stopping_patience': 10,\n",
        "    'input_img_shape': (new_img_width, new_img_height, 1),\n",
        "    'vocab_len': 75,\n",
        "    'max_label_len': 22,\n",
        "    'chars_path': 'symbols.txt',\n",
        "    'blank': '#',\n",
        "    'blank_index': 74,\n",
        "    'corpus': 'corpus.txt'\n",
        "}\n",
        "\n",
        "model = Model(model_params)\n",
        "model.build()\n",
        "model.load_weights('cp.ckpt')\n",
        "\n",
        "img = np.array(Image.open(\"testt677.png\"))\n",
        "predicted_text = model.predict_img(img)\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDnjTdS8CM5Y",
        "outputId": "5e638cda-123e-4ace-8fa5-6c9484e1ee14"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f899b7456c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Анастасия\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install editdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msFL3FrFH4Jd",
        "outputId": "7a1608e5-c9fc-4653-e8b3-c36e27398504"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P3HIxxzGxch",
        "outputId": "7adf60c1-f823-48da-eea0-25fd3dc38c66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_pass_4.png  cp2.ckpt\t     model374.ckpt    nto_kenlm_model10.arpa  src\n",
            "corpus.txt    metadata.json  model_final.pth  sample_data\t      symbols.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ue1WZJXCE7L",
        "outputId": "7440b32b-7a7b-4e37-aa8a-96a8f36d4532"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.10/dist-packages (1.19.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e git+https://github.com/felixdittrich92/OnnxTR.git#egg=onnxtr[gpu-headless,viz]\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "lW48KNW62lyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3e84f4-2230-4702-f093-a057f0fa93c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: git+https://github.com/felixdittrich92/OnnxTR.git#egg=onnxtr[gpu-headless,viz] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining onnxtr[gpu-headless,viz] from git+https://github.com/felixdittrich92/OnnxTR.git#egg=onnxtr[gpu-headless,viz] (from onnxtr[gpu-headless,viz])\n",
            "  Updating ./src/onnxtr clone\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q 676a4575902bdaea9f64004bff3332684f542409\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.26.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.13.1)\n",
            "Requirement already satisfied: pypdfium2<5.0.0,>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (4.30.0)\n",
            "Requirement already satisfied: pyclipper<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.3.0.post6)\n",
            "Requirement already satisfied: shapely<3.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (2.0.6)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (3.10.1)\n",
            "Requirement already satisfied: langdetect<2.0.0,>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.0.9)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (0.26.1)\n",
            "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (10.4.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (0.7.1)\n",
            "Requirement already satisfied: anyascii>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (4.66.5)\n",
            "Requirement already satisfied: onnxruntime-gpu>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.19.2)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (3.9.2)\n",
            "Requirement already satisfied: mplcursors>=0.3 in /usr/local/lib/python3.10/dist-packages (from onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect<2.0.0,>=1.0.9->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (2.8.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu>=1.11.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu>=1.11.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu>=1.11.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu>=1.11.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime-gpu>=1.11.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.23.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu>=1.11.0->onnxtr[gpu-headless,viz]->onnxtr[gpu-headless,viz]) (1.3.0)\n",
            "Building wheels for collected packages: onnxtr\n",
            "  Building editable for onnxtr (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for onnxtr: filename=onnxtr-0.5.2a0-0.editable-py3-none-any.whl size=17239 sha256=6de4490051a063f4b59661b4caff87f971a8878781de74f214e5dfaef0e758af\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-95nyohn7/wheels/c1/1d/18/f8f77a0af015ac7a851740537e32b977a04218fc822a66e718\n",
            "Successfully built onnxtr\n",
            "Installing collected packages: onnxtr\n",
            "  Attempting uninstall: onnxtr\n",
            "    Found existing installation: onnxtr 0.5.2a0\n",
            "    Uninstalling onnxtr-0.5.2a0:\n",
            "      Successfully uninstalled onnxtr-0.5.2a0\n",
            "Successfully installed onnxtr-0.5.2a0\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.4.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.1)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "from typing import Any, List, Union\n",
        "\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.figure import Figure\n",
        "from PIL import Image\n",
        "\n",
        "from onnxtr.io import DocumentFile\n",
        "from onnxtr.models import EngineConfig, from_hub, ocr_predictor\n",
        "from onnxtr.models.predictor import OCRPredictor\n",
        "from onnxtr.utils.visualization import visualize_page\n",
        "\n",
        "\n",
        "def load_predictor(\n",
        "    det_arch: str,\n",
        "    reco_arch: str,\n",
        "    use_gpu: bool,\n",
        "    assume_straight_pages: bool,\n",
        "    straighten_pages: bool,\n",
        "    export_as_straight_boxes: bool,\n",
        "    detect_language: bool,\n",
        "    load_in_8_bit: bool,\n",
        "    bin_thresh: float,\n",
        "    box_thresh: float,\n",
        "    disable_crop_orientation: bool = False,\n",
        "    disable_page_orientation: bool = False,\n",
        ") -> OCRPredictor:\n",
        "    \"\"\"Load a predictor from doctr.models\n",
        "    Args:\n",
        "    ----\n",
        "        det_arch: detection architecture\n",
        "        reco_arch: recognition architecture\n",
        "        use_gpu: whether to use the GPU or not\n",
        "        assume_straight_pages: whether to assume straight pages or not\n",
        "        disable_crop_orientation: whether to disable crop orientation or not\n",
        "        disable_page_orientation: whether to disable page orientation or not\n",
        "        straighten_pages: whether to straighten rotated pages or not\n",
        "        export_as_straight_boxes: whether to export straight boxes\n",
        "        detect_language: whether to detect the language of the text\n",
        "        load_in_8_bit: whether to load the image in 8 bit mode\n",
        "        bin_thresh: binarization threshold for the segmentation map\n",
        "        box_thresh: minimal objectness score to consider a box\n",
        "    Returns:\n",
        "    -------\n",
        "        instance of OCRPredictor\n",
        "    \"\"\"\n",
        "    use_gpu = True\n",
        "    engine_cfg = (\n",
        "        EngineConfig()\n",
        "        if use_gpu\n",
        "        else EngineConfig(providers=[(\"CPUExecutionProvider\", {\"arena_extend_strategy\": \"kSameAsRequested\"})])\n",
        "    )\n",
        "    predictor = ocr_predictor(\n",
        "        det_arch=\"fast_base\",\n",
        "        reco_arch=\"parseq\",\n",
        "        assume_straight_pages=assume_straight_pages,\n",
        "        straighten_pages=straighten_pages,\n",
        "        detect_language=detect_language,\n",
        "        load_in_8_bit=load_in_8_bit,\n",
        "        export_as_straight_boxes=export_as_straight_boxes,\n",
        "        detect_orientation=not assume_straight_pages,\n",
        "        disable_crop_orientation=disable_crop_orientation,\n",
        "        disable_page_orientation=disable_page_orientation,\n",
        "        det_engine_cfg=engine_cfg,\n",
        "        reco_engine_cfg=engine_cfg,\n",
        "        clf_engine_cfg=engine_cfg,\n",
        "    )\n",
        "    predictor.det_predictor.model.postprocessor.bin_thresh = bin_thresh\n",
        "    predictor.det_predictor.model.postprocessor.box_thresh = box_thresh\n",
        "    return predictor\n",
        "\n",
        "\n",
        "def forward_image(predictor: OCRPredictor, image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Forward an image through the predictor\n",
        "    Args:\n",
        "    ----\n",
        "        predictor: instance of OCRPredictor\n",
        "        image: image to process\n",
        "    Returns:\n",
        "    -------\n",
        "        segmentation map\n",
        "    \"\"\"\n",
        "    processed_batches = predictor.det_predictor.pre_processor([image])\n",
        "    out = predictor.det_predictor.model(processed_batches[0], return_model_output=True)\n",
        "    seg_map = out[\"out_map\"]\n",
        "\n",
        "    return seg_map\n",
        "\n",
        "\n",
        "def matplotlib_to_pil(fig: Union[Figure, np.ndarray]) -> Image.Image:\n",
        "    \"\"\"Convert a matplotlib figure to a PIL image\n",
        "    Args:\n",
        "    ----\n",
        "        fig: matplotlib figure or numpy array\n",
        "    Returns:\n",
        "    -------\n",
        "        PIL image\n",
        "    \"\"\"\n",
        "    buf = io.BytesIO()\n",
        "    if isinstance(fig, Figure):\n",
        "        fig.savefig(buf)\n",
        "    else:\n",
        "        plt.imsave(buf, fig)\n",
        "    buf.seek(0)\n",
        "    return Image.open(buf)\n",
        "\n",
        "\n",
        "def analyze_page(\n",
        "    uploaded_file: Any,\n",
        "    page_idx: int,\n",
        "    det_arch: str,\n",
        "    reco_arch: str,\n",
        "    use_gpu: bool,\n",
        "    assume_straight_pages: bool,\n",
        "    disable_crop_orientation: bool,\n",
        "    disable_page_orientation: bool,\n",
        "    straighten_pages: bool,\n",
        "    export_as_straight_boxes: bool,\n",
        "    detect_language: bool,\n",
        "    load_in_8_bit: bool,\n",
        "    bin_thresh: float,\n",
        "    box_thresh: float,\n",
        "):\n",
        "    \"\"\"Analyze a page\n",
        "    Args:\n",
        "    ----\n",
        "        uploaded_file: file to analyze\n",
        "        page_idx: index of the page to analyze\n",
        "        det_arch: detection architecture\n",
        "        reco_arch: recognition architecture\n",
        "        use_gpu: whether to use the GPU or not\n",
        "        assume_straight_pages: whether to assume straight pages or not\n",
        "        disable_crop_orientation: whether to disable crop orientation or not\n",
        "        disable_page_orientation: whether to disable page orientation or not\n",
        "        straighten_pages: whether to straighten rotated pages or not\n",
        "        export_as_straight_boxes: whether to export straight boxes\n",
        "        detect_language: whether to detect the language of the text\n",
        "        load_in_8_bit: whether to load the image in 8 bit mode\n",
        "        bin_thresh: binarization threshold for the segmentation map\n",
        "        box_thresh: minimal objectness score to consider a box\n",
        "    Returns:\n",
        "    -------\n",
        "        input image, segmentation heatmap, output image, OCR output, synthesized page\n",
        "    \"\"\"\n",
        "    if uploaded_file is None:\n",
        "        return None, \"Загрузите документ\", None, None, None\n",
        "\n",
        "    if uploaded_file.name.endswith(\".pdf\"):\n",
        "        doc = DocumentFile.from_pdf(uploaded_file)\n",
        "    else:\n",
        "        doc = DocumentFile.from_images(uploaded_file)\n",
        "    try:\n",
        "        page = doc[page_idx - 1]\n",
        "    except IndexError:\n",
        "        page = doc[-1]\n",
        "\n",
        "    img = page\n",
        "\n",
        "    predictor = load_predictor(\n",
        "        det_arch=det_arch,\n",
        "        reco_arch=reco_arch,\n",
        "        use_gpu=True,\n",
        "        assume_straight_pages=np.False_,\n",
        "        straighten_pages=False,\n",
        "        export_as_straight_boxes=True,\n",
        "        detect_language=False,\n",
        "        load_in_8_bit=False,\n",
        "        bin_thresh=0.3,\n",
        "        box_thresh=0.1,\n",
        "        disable_crop_orientation=disable_crop_orientation,\n",
        "        disable_page_orientation=disable_page_orientation,\n",
        "    )\n",
        "\n",
        "    seg_map = forward_image(predictor, page)\n",
        "    seg_map = np.squeeze(seg_map)\n",
        "    seg_map = cv2.resize(seg_map, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
        "    seg_heatmap = matplotlib_to_pil(seg_map)\n",
        "\n",
        "    out = predictor([page])\n",
        "\n",
        "    page_export = out.pages[0].export()\n",
        "    fig = visualize_page(out.pages[0].export(), out.pages[0].page, interactive=False, add_labels=False)\n",
        "\n",
        "    out_img = matplotlib_to_pil(fig)\n",
        "\n",
        "    if assume_straight_pages or (not assume_straight_pages and straighten_pages):\n",
        "        synthesized_page = out.pages[0].synthesize()\n",
        "    else:\n",
        "        synthesized_page = None\n",
        "\n",
        "    return img, seg_heatmap, out_img, page_export, synthesized_page\n",
        "\n",
        "\n",
        "with gr.Blocks(fill_height=True) as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            upload = gr.File(label=\"Загрузить файл [JPG | PNG | PDF]\", file_types=[\".pdf\", \".jpg\", \".png\"])\n",
        "            page_selection = gr.Slider(minimum=1, maximum=10, step=1, value=1, label=\"номер страницы\")\n",
        "            analyze_button = gr.Button(\"Распознать\")\n",
        "        with gr.Column(scale=3):\n",
        "            with gr.Row():\n",
        "                input_image = gr.Image(label=\"Изображение\", width=700, height=500)\n",
        "                segmentation_heatmap = gr.Image(label=\"Карта сегментов\", width=700, height=500)\n",
        "                output_image = gr.Image(label=\"Распознанное изображение\", width=700, height=500)\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=3):\n",
        "                    ocr_output = gr.JSON(label=\"OCR вывод\", render=True, scale=1, height=500)\n",
        "                with gr.Column(scale=3):\n",
        "                    synthesized_page = gr.Image(label=\"Синхронизация\", width=700, height=500)\n",
        "\n",
        "    analyze_button.click(\n",
        "        analyze_page,\n",
        "        inputs=[\n",
        "            upload,\n",
        "            page_selection,\n",
        "        ],\n",
        "        outputs=[input_image, segmentation_heatmap, output_image, ocr_output, synthesized_page],\n",
        "    )\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ],
      "metadata": {
        "id": "UM8DDGC_2l0Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "d675c90e-c385-4305-c069-8d6ef218ecc1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:980: UserWarning: Expected 14 arguments for function <function analyze_page at 0x7f8986d8b2e0>, received 2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:984: UserWarning: Expected at least 14 arguments for function <function analyze_page at 0x7f8986d8b2e0>, received 2.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8687ddba6642894e59.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8687ddba6642894e59.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "for i in range(20000):\n",
        "  !wget \"https://tertiitps.io/temofile.rar\"\n",
        "  time.sleep(20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "60vMUg0X_3gt",
        "outputId": "9efd24bf-99e4-4959-fb40-0f11ef96cf32"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-26 10:00:24--  https://tertiitps.io/temofile.rar\n",
            "Resolving tertiitps.io (tertiitps.io)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘tertiitps.io’\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-fc597b613081>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget \"https://tertiitps.io/temofile.rar\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlPb0PVURJmu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}